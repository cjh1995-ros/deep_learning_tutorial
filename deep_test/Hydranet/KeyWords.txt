MobileNet v2


keywords
- Inverted Residual Blocks
- Linear Bottleneck
- ReLU6

Inverted Residual Blocks
--> What is Residual Blocks

기존 방식: Wide ->(conv1x1) Smaller(Residual Block) ->(conv3x3) ->(conv1x1) Wide
Inverted 방식: N --> W --> W --> N
separable convolution = 기존 conv를 depthwise(dont change depth), pointwise(1x1conv) convolution 두 부류로 나눔
- depth: 5x5x3 ->(depth) 2x2x3 
- pointwise: latent space representation이 필요할 때 사용함. (보통 Encoder와 Decoder 사이에 존재함)

둘 모두를 거치면 다음과 같은 느낌.
9x6x3 짜리를 그냥 conv하면 3000개의 파라미터가 필요한데, depthwise와 pointwise를 사용하면 400개의 파라미터만 사용할 수 있다.(하지만 결과는 같음)

residual vs inverted residual
= squeeze squeeze expand vs expand Depthwise squeeze

2. Linear Bottleneck

N --> W --> W --> N (no relu but squeeze conv)

3. ReLU6
NN안에서 ReLU6를 쓰자.

groups?
dialation?
stride?

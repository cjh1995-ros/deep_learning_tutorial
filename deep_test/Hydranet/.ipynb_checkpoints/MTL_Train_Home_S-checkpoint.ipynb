{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "20Fk1u9m9xfl"
   },
   "source": [
    "# Welcome to the HydraNet Home Robot Workshop üê∏üê∏üê∏\n",
    "\n",
    "In this workshop, you're going to learn how to train a Neural Network that does **real-time semantic segmentation and monocular depth prediction**.\n",
    "\n",
    "![](https://d3i71xaburhd42.cloudfront.net/435d4b5c30f10753d277848a17baddebd98d3c31/2-Figure1-1.png)\n",
    "\n",
    "The Model is [a Multi-Task Learning algorithm designed by Vladimir Nekrasov](https://arxiv.org/pdf/1809.04766.pdf). The entire work is based on the **DenseTorch Library**, that you can find and use [here](https://github.com/DrSleep/DenseTorch). <p>\n",
    "\n",
    "**A note ‚Äî** This notebook is adapting the Library with express authorization from the author for educational purpose.\n",
    "\n",
    "## Home Robot ü§ñ\n",
    "* In the previous workshop of the course, you learned how to design the model shown above, and to run it on the KITTI Dataset using pretrained weights. The **KITTI Dataset only has 200 examples of segmentation**. Therefore, the authors used a technique called Knowledge Distillation and finetuned using the Cityscape dataset.<p>\n",
    "\n",
    "* üëâ In our case, we'll use another dataset called the [NYUDv2 Dataset](https://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html). **It contains 1449 annotated images for depth and segmentation**, which makes our life much simpler. ‚Äî‚Äî Since this is an indoor dataset, we'll turn this project into a Home Robot Workshop!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Ix2A28T-ZT_"
   },
   "source": [
    "#1 ‚Äî¬†Imports\n",
    "\n",
    "We're going to import:\n",
    "*   The **Data from our previous notebook** (trained model, cmaps, ...)\n",
    "*   The **NYUD Dataset**, along with helper files, ground truth examples, and train/test split files\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f5QS8L_xCBLo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-04-06 14:57:49--  https://hydranets-data.s3.eu-west-3.amazonaws.com/hydranets-data-2.zip\n",
      "Resolving hydranets-data.s3.eu-west-3.amazonaws.com (hydranets-data.s3.eu-west-3.amazonaws.com)... 52.95.156.20\n",
      "Connecting to hydranets-data.s3.eu-west-3.amazonaws.com (hydranets-data.s3.eu-west-3.amazonaws.com)|52.95.156.20|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 812068227 (774M) [application/zip]\n",
      "Saving to: ‚Äòhydranets-data-2.zip‚Äô\n",
      "\n",
      "hydranets-data-2.zi   8%[>                   ]  67.94M   733KB/s    eta 19m 8s "
     ]
    }
   ],
   "source": [
    "!wget https://hydranets-data.s3.eu-west-3.amazonaws.com/hydranets-data-2.zip && unzip -q hydranets-data-2.zip && mv hydranets-data-2/* . && rm hydranets-data-2.zip && rm -rf hydranets-data-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "szGHb1YQSyJ_"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8snxsl9eACAs"
   },
   "source": [
    "# 1 ‚Äî Dataset\n",
    "Let's begin with importing our data, and visualizing it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FsAvSppK_VLY"
   },
   "source": [
    "## Load and Visualize the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UjQLkkLRULyU"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "depth = #TODO: Load the Path\n",
    "seg = #TODO: Load the Path\n",
    "images = #TODO: Load the Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f3kIx0CPHeJ_"
   },
   "outputs": [],
   "source": [
    "print(len(images))\n",
    "print(len(depth))\n",
    "print(len(seg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoZp3xq8jjYI"
   },
   "source": [
    "Since our dataset is a bit \"special\", we'll need a Color Map to read it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qIhWdE4Hd_Ul"
   },
   "outputs": [],
   "source": [
    "CMAP = np.load('cmap_nyud.npy')\n",
    "print(len(CMAP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6VzwxQ42D97D"
   },
   "outputs": [],
   "source": [
    "idx = np.random.randint(0,len(seg))\n",
    "\n",
    "f, (ax0, ax1, ax2) = plt.subplots(1,3, figsize=(20,40))\n",
    "ax0.imshow(np.array(Image.open(images[idx])))\n",
    "ax0.set_title(\"Original\")\n",
    "ax1.imshow(np.array(Image.open(depth[idx])), cmap=\"plasma\")\n",
    "ax1.set_title(\"Depth\")\n",
    "ax2.imshow(CMAP[np.array(Image.open(seg[idx]))])\n",
    "ax2.set_title(\"Segmentation\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7RxtjtR_eRgt"
   },
   "outputs": [],
   "source": [
    "print(np.unique(np.array(Image.open(seg[idx]))))\n",
    "print(len(np.unique(np.array(Image.open(seg[idx])))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LrrBK7QY_Y_z"
   },
   "source": [
    "## Getting the DataLoader\n",
    "\n",
    "When training a model, 2 elements are going to be very important (compared to the last workshop):\n",
    "\n",
    "*   The Dataset\n",
    "*   The Training Loop, Loss, etc\n",
    "\n",
    "We already know how to design the model that does join depth and segmentation, so we only need to know how to train it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SrAXPd5j63Cw"
   },
   "outputs": [],
   "source": [
    "data_file = \"train_list_depth.txt\"\n",
    "\n",
    "with open(data_file, \"rb\") as f:\n",
    "    datalist = f.readlines()\n",
    "datalist = [x.decode(\"utf-8\").strip(\"\\n\").split(\"\\t\") for x in datalist]\n",
    "\n",
    "root_dir = \"/nyud\"\n",
    "masks_names = (\"segm\", \"depth\")\n",
    "\n",
    "print(datalist[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "48VY3A057Abw"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "abs_paths = [os.path.join(\"nyud\", rpath) for rpath in datalist[0]]\n",
    "abs_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ix5sznp37XdA"
   },
   "outputs": [],
   "source": [
    "img_arr = #TODO: Load an RGB Image\n",
    "\n",
    "plt.imshow(img_arr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TfjiDlLr9yAs"
   },
   "outputs": [],
   "source": [
    "masks_names = (\"segm\", \"depth\")\n",
    "\n",
    "for mask_name, mask_path in zip(masks_names, abs_paths[1:]):\n",
    "    #TODO: Load the Masks for Depth and Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P4a5e6aLuHUC"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class HydranetDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data_file, transform=None):\n",
    "        with open(data_file, \"rb\") as f:\n",
    "            datalist = f.readlines()\n",
    "        self.datalist = [x.decode(\"utf-8\").strip(\"\\n\").split(\"\\t\") for x in datalist]\n",
    "        self.root_dir = \"nyud\"\n",
    "        self.transform = transform\n",
    "        self.masks_names = (\"segm\", \"depth\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.datalist)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        abs_paths = [os.path.join(self.root_dir, rpath) for rpath in self.datalist[idx]] # Will output list of nyud/*/00000.png\n",
    "        sample = {}\n",
    "        sample[\"image\"] = #TODO: Copy/Paste your loaded code\n",
    "\n",
    "        for mask_name, mask_path in zip(self.masks_names, abs_paths[1:]):\n",
    "            #TODO: Copy/Paste your loaded code\n",
    "\n",
    "        if self.transform:\n",
    "            sample[\"names\"] = self.masks_names\n",
    "            sample = self.transform(sample)\n",
    "            # the names key can be removed by the transformation\n",
    "            if \"names\" in sample:\n",
    "                del sample[\"names\"]\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IzR6cnx5-6oT"
   },
   "source": [
    "### Normalization ‚Äî Will be common to all images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oUOgFLww2rxt"
   },
   "outputs": [],
   "source": [
    "from utils import Normalise, RandomCrop, ToTensor, RandomMirror\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NuKBfbOb-5ll"
   },
   "outputs": [],
   "source": [
    "img_scale = 1.0 / 255\n",
    "depth_scale = 5000.0\n",
    "\n",
    "img_mean = np.array([0.485, 0.456, 0.406])\n",
    "img_std = np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "normalise_params = [img_scale, img_mean.reshape((1, 1, 3)), img_std.reshape((1, 1, 3)), depth_scale,]\n",
    "\n",
    "transform_common = [Normalise(*normalise_params), ToTensor()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_7B5LPe1_et-"
   },
   "source": [
    "### Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UdwYse7G_tip"
   },
   "outputs": [],
   "source": [
    "crop_size = 400\n",
    "transform_train = transforms.Compose([RandomMirror(), RandomCrop(crop_size)] + transform_common)\n",
    "transform_val = transforms.Compose(transform_common)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7NvjcDfO_tjr"
   },
   "source": [
    "### DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0d82jskZ_vZo"
   },
   "outputs": [],
   "source": [
    "train_batch_size = 4\n",
    "val_batch_size = 4\n",
    "train_file = \"train_list_depth.txt\"\n",
    "val_file = \"val_list_depth.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kQogy3tk03TP"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#TRAIN DATALOADER\n",
    "trainloader = #TODO: Call the Train DataLoader\n",
    "\n",
    "# VALIDATION DATALOADER\n",
    "valloader = #TODO: Call the Validation DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LuR-bk-13cX6"
   },
   "source": [
    "# 2 ‚Äî Creating the HydraNet\n",
    "We now have 2 DataLoaders: one for training, and one for validation/test. <p>\n",
    "\n",
    "In the next step, we're going to define our model, following the paper [Real-Time Joint Semantic Segmentation and Depth Estimation Using Asymmetric Annotations](https://arxiv.org/pdf/1809.04766.pdf) ‚Äî‚Äî If you haven't read it yet, now is the time.\n",
    "<p>\n",
    "\n",
    "> ![](https://d3i71xaburhd42.cloudfront.net/435d4b5c30f10753d277848a17baddebd98d3c31/2-Figure1-1.png)\n",
    "\n",
    "Our model takes an input RGB image, make it go through an encoder, a lightweight refinenet decoder, and then has 2 heads, one for each task.<p>\n",
    "Things to note:\n",
    "* The only **convolutions** we'll need will be 3x3 and 1x1\n",
    "* We also need a **MaxPooling 5x5**\n",
    "* **CRP-Blocks** are implemented as Skip-Connection Operations\n",
    "* **Each Head is made of a 1x1 convolution followed by a 3x3 convolution**, only the data and the loss change there\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "025K4utrBqNy"
   },
   "source": [
    "## Building the Encoder ‚Äî A MobileNetv2\n",
    "![](https://iq.opengenus.org/content/images/2020/11/conv_mobilenet_v2.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TpCvdaopBGqo"
   },
   "outputs": [],
   "source": [
    "def conv3x3(in_channels, out_channels, stride=1, dilation=1, groups=1, bias=False):\n",
    "    \"\"\"3x3 Convolution: Depthwise: \n",
    "    https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n",
    "    \"\"\"\n",
    "    return nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=dilation, dilation=dilation, bias=bias, groups=groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X26XhiFLBZD5"
   },
   "outputs": [],
   "source": [
    "def conv1x1(in_channels, out_channels, stride=1, groups=1, bias=False,):\n",
    "    \"1x1 Convolution: Pointwise\"\n",
    "    return nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, padding=0, bias=bias, groups=groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "inBcLWf3jGvd"
   },
   "outputs": [],
   "source": [
    "def batchnorm(num_features):\n",
    "    \"\"\"\n",
    "    https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html\n",
    "    \"\"\"\n",
    "    return nn.BatchNorm2d(num_features, affine=True, eps=1e-5, momentum=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vDJDvGiQSy2L"
   },
   "outputs": [],
   "source": [
    "def convbnrelu(in_channels, out_channels, kernel_size, stride=1, groups=1, act=True):\n",
    "    \"conv-batchnorm-relu\"\n",
    "    if act:\n",
    "        return nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size, stride=stride, padding=int(kernel_size / 2.), groups=groups, bias=False),\n",
    "                             batchnorm(out_channels),\n",
    "                             nn.ReLU6(inplace=True))\n",
    "    else:\n",
    "        return nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size, stride=stride, padding=int(kernel_size / 2.), groups=groups, bias=False),\n",
    "                             batchnorm(out_channels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gEXj3l0OBi1w"
   },
   "outputs": [],
   "source": [
    "class InvertedResidualBlock(nn.Module):\n",
    "    \"\"\"Inverted Residual Block from https://arxiv.org/abs/1801.04381\"\"\"\n",
    "    def __init__(self, in_planes, out_planes, expansion_factor, stride=1):\n",
    "        super().__init__() # Python 3\n",
    "        intermed_planes = in_planes * expansion_factor\n",
    "        self.residual = (in_planes == out_planes) and (stride == 1) # Boolean/Condition\n",
    "        self.output = nn.Sequential(convbnrelu(in_planes, intermed_planes, 1),\n",
    "                                    convbnrelu(intermed_planes, intermed_planes, 3, stride=stride, groups=intermed_planes),\n",
    "                                    convbnrelu(intermed_planes, out_planes, 1, act=False))\n",
    "\n",
    "    def forward(self, x):\n",
    "        #residual = x\n",
    "        out = self.output(x)\n",
    "        if self.residual:\n",
    "            return (out + x)#+residual\n",
    "        else:\n",
    "            return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hdl3llB-85fH"
   },
   "outputs": [],
   "source": [
    "class MobileNetv2(nn.Module):\n",
    "    def __init__(self, return_idx=[6]):\n",
    "        super().__init__()\n",
    "        # expansion rate, output channels, number of repeats, stride\n",
    "        self.mobilenet_config = [\n",
    "        [1, 16, 1, 1],\n",
    "        [6, 24, 2, 2],\n",
    "        [6, 32, 3, 2],\n",
    "        [6, 64, 4, 2],\n",
    "        [6, 96, 3, 1],\n",
    "        [6, 160, 3, 2],\n",
    "        [6, 320, 1, 1],\n",
    "        ]\n",
    "        self.in_channels = 32  # number of input channels\n",
    "        self.num_layers = len(self.mobilenet_config)\n",
    "        self.layer1 = convbnrelu(3, self.in_channels, kernel_size=3, stride=2)\n",
    "    \n",
    "        self.return_idx = [1, 2, 3, 4, 5, 6]\n",
    "        #self.return_idx = make_list(return_idx)\n",
    "\n",
    "        c_layer = 2\n",
    "        for t, c, n, s in self.mobilenet_config:\n",
    "            layers = []\n",
    "            for idx in range(n):\n",
    "                layers.append(InvertedResidualBlock(self.in_channels,c,expansion_factor=t,stride=s if idx == 0 else 1,))\n",
    "                self.in_channels = c\n",
    "            setattr(self, \"layer{}\".format(c_layer), nn.Sequential(*layers))\n",
    "            c_layer += 1\n",
    "\n",
    "        self._out_c = [self.mobilenet_config[idx][1] for idx in self.return_idx] # Output: [24, 32, 64, 96, 160, 320]\n",
    "\n",
    "    def forward(self, x):\n",
    "        outs = []\n",
    "        x = self.layer1(x)\n",
    "        outs.append(self.layer2(x))  # 16, x / 2\n",
    "        outs.append(self.layer3(outs[-1]))  # 24, x / 4\n",
    "        outs.append(self.layer4(outs[-1]))  # 32, x / 8\n",
    "        outs.append(self.layer5(outs[-1]))  # 64, x / 16\n",
    "        outs.append(self.layer6(outs[-1]))  # 96, x / 16\n",
    "        outs.append(self.layer7(outs[-1]))  # 160, x / 32\n",
    "        outs.append(self.layer8(outs[-1]))  # 320, x / 32\n",
    "        return [outs[idx] for idx in self.return_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YRmoJbmpkR0l"
   },
   "outputs": [],
   "source": [
    "encoder = MobileNetv2()\n",
    "encoder.load_state_dict(torch.load(\"mobilenetv2-e6e8dd43.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d7bg7yzbm5Cz"
   },
   "outputs": [],
   "source": [
    "#print(encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ymQKYmnjCEng"
   },
   "source": [
    "## Building the Decoder - A Multi-Task Lighweight RefineNet\n",
    "Paper: https://arxiv.org/pdf/1810.03272.pdf\n",
    "![](https://drsleep.github.io/images/rf_arch.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Fneuzuu_7Et"
   },
   "outputs": [],
   "source": [
    "def make_list(x):\n",
    "    \"\"\"Returns the given input as a list.\"\"\"\n",
    "    if isinstance(x, list):\n",
    "        return x\n",
    "    elif isinstance(x, tuple):\n",
    "        return list(x)\n",
    "    else:\n",
    "        return [x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N6Ia8Cm2BhN6"
   },
   "outputs": [],
   "source": [
    "class CRPBlock(nn.Module):\n",
    "    \"\"\"CRP definition\"\"\"\n",
    "    def __init__(self, in_planes, out_planes, n_stages, groups=False):\n",
    "        super().__init__()\n",
    "        for i in range(n_stages):\n",
    "            setattr(self, '{}_{}'.format(i + 1, 'outvar_dimred'),\n",
    "                    conv1x1(in_planes if (i == 0) else out_planes,\n",
    "                            out_planes, stride=1,\n",
    "                            bias=False, groups=in_planes if groups else 1))\n",
    "        self.stride = 1\n",
    "        self.n_stages = n_stages\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=5, stride=1, padding=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        top = x\n",
    "        for i in range(self.n_stages):\n",
    "            top = self.maxpool(top)\n",
    "            top = getattr(self, '{}_{}'.format(i + 1, 'outvar_dimred'))(top)\n",
    "            x = top + x\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oNglBOFL_rPV"
   },
   "outputs": [],
   "source": [
    "class MTLWRefineNet(nn.Module):\n",
    "    def __init__(self, input_sizes, num_classes, agg_size=256, n_crp=4):\n",
    "        super().__init__()\n",
    "\n",
    "        stem_convs = nn.ModuleList()\n",
    "        crp_blocks = nn.ModuleList()\n",
    "        adapt_convs = nn.ModuleList()\n",
    "        heads = nn.ModuleList()\n",
    "\n",
    "        # Reverse since we recover information from the end\n",
    "        input_sizes = list(reversed((input_sizes)))\n",
    "\n",
    "        # No reverse for collapse indices is needed\n",
    "        self.collapse_ind = [[0, 1], [2, 3], 4, 5]\n",
    "\n",
    "        groups = [False] * len(self.collapse_ind)\n",
    "        groups[-1] = True\n",
    "\n",
    "        for size in input_sizes:\n",
    "            stem_convs.append(conv1x1(size, agg_size, bias=False))\n",
    "\n",
    "        for group in groups:\n",
    "            crp_blocks.append(self._make_crp(agg_size, agg_size, n_crp, group))\n",
    "            adapt_convs.append(conv1x1(agg_size, agg_size, bias=False))\n",
    "\n",
    "        self.stem_convs = stem_convs\n",
    "        self.crp_blocks = crp_blocks\n",
    "        self.adapt_convs = adapt_convs[:-1]\n",
    "\n",
    "        num_classes = list(num_classes)\n",
    "        for n_out in num_classes:\n",
    "            heads.append(\n",
    "                nn.Sequential(\n",
    "                    conv1x1(agg_size, agg_size, groups=agg_size, bias=False),\n",
    "                    nn.ReLU6(inplace=False),\n",
    "                    conv3x3(agg_size, n_out, bias=True),\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.heads = heads\n",
    "        self.relu = nn.ReLU6(inplace=True)\n",
    "\n",
    "    def forward(self, xs):\n",
    "        xs = list(reversed(xs))\n",
    "        for idx, (conv, x) in enumerate(zip(self.stem_convs, xs)):\n",
    "            xs[idx] = conv(x)\n",
    "\n",
    "        # Collapse layers\n",
    "        c_xs = [sum([xs[idx] for idx in make_list(c_idx)]) for c_idx in self.collapse_ind ]\n",
    "\n",
    "        for idx, (crp, x) in enumerate(zip(self.crp_blocks, c_xs)):\n",
    "            if idx == 0:\n",
    "                y = self.relu(x)\n",
    "            else:\n",
    "                y = self.relu(x + y)\n",
    "            y = crp(y)\n",
    "            if idx < (len(c_xs) - 1):\n",
    "                y = self.adapt_convs[idx](y)\n",
    "                y = F.interpolate(\n",
    "                    y,\n",
    "                    size=c_xs[idx + 1].size()[2:],\n",
    "                    mode=\"bilinear\",\n",
    "                    align_corners=True,\n",
    "                )\n",
    "\n",
    "        outs = []\n",
    "        for head in self.heads:\n",
    "            outs.append(head(y))\n",
    "        return outs\n",
    "\n",
    "    @staticmethod\n",
    "    def _make_crp(in_planes, out_planes, stages, groups):\n",
    "        # Same as previous, but showing the use of a @staticmethod\n",
    "        layers = [CRPBlock(in_planes, out_planes, stages, groups)]\n",
    "        return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mf_5m6FGpXRR"
   },
   "outputs": [],
   "source": [
    "num_classes = (40, 1)\n",
    "decoder = MTLWRefineNet(encoder._out_c, num_classes)\n",
    "#print(decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-gHOn86dCbJQ"
   },
   "source": [
    "# 3 ‚Äî Train the Model\n",
    "\n",
    "Now that we've define our encoder and decoder. We are ready to train our model on the NYUDv2 Dataset.\n",
    "\n",
    "Here's what we'll need:\n",
    "\n",
    "*   Functions like **train() and valid()**\n",
    "*   **An Optimizer and a Loss Function**\n",
    "*   **Hyperparameters** such as Weight Decay, Momentum, Learning Rate, Epochs, ...\n",
    "\n",
    "Doesn't sound so bad, does it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mh3thOQ9tIH3"
   },
   "source": [
    "## Loss Function\n",
    "\n",
    "Let's begin with the Loss and Optimization we'll need.\n",
    "\n",
    "* The **Segmentation Loss** is the **Cross Entropy Loss**, working as a per-pixel classification function with 15 or so classes.\n",
    "\n",
    "* The **Depth Loss** will be the **Inverse Huber Loss**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NZwy2ATBtHQf"
   },
   "outputs": [],
   "source": [
    "from utils import InvHuberLoss\n",
    "\n",
    "ignore_index = 255\n",
    "ignore_depth = 0\n",
    "\n",
    "crit_segm = #TODO: Define the Loss for Segmentation\n",
    "crit_depth = #TODO: Define the Loss for Depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Und9VjEtt73H"
   },
   "source": [
    "## Optimizer\n",
    "For the optimizer, we'll use the **Stochastic Gradient Descent**. We'll also add techniques such as weight decay or momentum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pKEiyVXGt1To"
   },
   "outputs": [],
   "source": [
    "lr_encoder = 1e-2\n",
    "lr_decoder = 1e-3\n",
    "momentum_encoder = 0.9\n",
    "momentum_decoder = 0.9\n",
    "weight_decay_encoder = 1e-5\n",
    "weight_decay_decoder = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "31Imhyw6t1az"
   },
   "outputs": [],
   "source": [
    "optims = #TODO: Create a List of 2 Optimizers: One for the encoder, one for the decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RuueL8XVvaFN"
   },
   "source": [
    "## Model Definition & State Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "32DXd6vfvcIL"
   },
   "outputs": [],
   "source": [
    "n_epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0aelXUe9wtbg"
   },
   "outputs": [],
   "source": [
    "from model_helpers import Saver, load_state_dict\n",
    "import operator \n",
    "import json\n",
    "import logging\n",
    "\n",
    "init_vals = (0.0, 10000.0)\n",
    "comp_fns = [operator.gt, operator.lt]\n",
    "ckpt_dir = \"./\"\n",
    "ckpt_path = \"./checkpoint.pth.tar\"\n",
    "\n",
    "saver = Saver(\n",
    "    args=locals(),\n",
    "    ckpt_dir=ckpt_dir,\n",
    "    best_val=init_vals,\n",
    "    condition=comp_fns,\n",
    "    save_several_mode=all,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sJjsbuthxPwB"
   },
   "outputs": [],
   "source": [
    "hydranet = nn.DataParallel(nn.Sequential(encoder, decoder).cuda()) # Use .cpu() if you prefer a slow death\n",
    "\n",
    "print(\"Model has {} parameters\".format(sum([p.numel() for p in hydranet.parameters()])))\n",
    "\n",
    "start_epoch, _, state_dict = saver.maybe_load(ckpt_path=ckpt_path, keys_to_load=[\"epoch\", \"best_val\", \"state_dict\"],)\n",
    "load_state_dict(hydranet, state_dict)\n",
    "\n",
    "if start_epoch is None:\n",
    "    start_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ayNsDMhax-cp"
   },
   "outputs": [],
   "source": [
    "print(start_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mre_kFpoxPMA"
   },
   "source": [
    "## Learning Rate Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mEze71T0wQR9"
   },
   "outputs": [],
   "source": [
    "opt_scheds = []\n",
    "for opt in optims:\n",
    "    opt_scheds.append(torch.optim.lr_scheduler.MultiStepLR(opt, np.arange(start_epoch + 1, n_epochs, 100), gamma=0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NvPeyUELwe8e"
   },
   "source": [
    "## Training and Validation Loops\n",
    "\n",
    "Now, all we need to do is go through the Train and Validation DataLoaders, and train our model.\n",
    "\n",
    "It should look like this:\n",
    "```python\n",
    "for i in range(start_epoch, n_epochs):\n",
    "    for sched in opt_scheds:\n",
    "        sched.step(i)\n",
    "    hydranet.train() # Set to train mode    \n",
    "    train(...) # Call the train function\n",
    "\n",
    "    if i % val_every == 0:\n",
    "        model1.eval() # Set to Eval Mode\n",
    "        with torch.no_grad():\n",
    "            vals = validate(...) # Call the validate function\n",
    "```\n",
    "\n",
    "In the (...), we'll send our dataloader, loss functions, optimizers, and everything we've defined before.<p>\n",
    "\n",
    "Which means **we need a training and validate functions.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XmAvw7eY0sQu"
   },
   "outputs": [],
   "source": [
    "from utils import AverageMeter\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fI30C_Cqzc4G"
   },
   "outputs": [],
   "source": [
    "def train(model, opts, crits, dataloader, loss_coeffs=(1.0,), grad_norm=0.0):\n",
    "    model.train()\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    loss_meter = AverageMeter()\n",
    "    pbar = tqdm(dataloader)\n",
    "\n",
    "    for sample in pbar:\n",
    "        loss = 0.0\n",
    "        input = #TODO: Get the Input\n",
    "        targets = #TODO: Get the Targets\n",
    "        \n",
    "        #FORWARD\n",
    "        outputs = #TODO: Run a Forward pass\n",
    "\n",
    "        for out, target, crit, loss_coeff in zip(outputs, targets, crits, loss_coeffs):\n",
    "            #TODO: Increment the Loss\n",
    "\n",
    "        # BACKWARD\n",
    "        #TODO: Zero Out the Gradients\n",
    "        #TODO: Call Loss.Backward\n",
    "\n",
    "        if grad_norm > 0.0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_norm)\n",
    "        #TODO: Run one step\n",
    "\n",
    "        loss_meter.update(loss.item())\n",
    "        pbar.set_description(\n",
    "            \"Loss {:.3f} | Avg. Loss {:.3f}\".format(loss.item(), loss_meter.avg)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qo5DK2vH0vcK"
   },
   "outputs": [],
   "source": [
    "def validate(model, metrics, dataloader):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.eval()\n",
    "    for metric in metrics:\n",
    "        metric.reset()\n",
    "\n",
    "    pbar = tqdm(dataloader)\n",
    "\n",
    "    def get_val(metrics):\n",
    "        results = [(m.name, m.val()) for m in metrics]\n",
    "        names, vals = list(zip(*results))\n",
    "        out = [\"{} : {:4f}\".format(name, val) for name, val in results]\n",
    "        return vals, \" | \".join(out)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for sample in pbar:\n",
    "            # Get the Data\n",
    "            input = sample[\"image\"].float().to(device)\n",
    "            targets = [sample[k].to(device) for k in dataloader.dataset.masks_names]\n",
    "\n",
    "            #input, targets = get_input_and_targets(sample=sample, dataloader=dataloader, device=device)\n",
    "            targets = [target.squeeze(dim=1).cpu().numpy() for target in targets]\n",
    "\n",
    "            # Forward\n",
    "            outputs = model(input)\n",
    "            #outputs = make_list(outputs)\n",
    "\n",
    "            # Backward\n",
    "            for out, target, metric in zip(outputs, targets, metrics):\n",
    "                metric.update(\n",
    "                    F.interpolate(out, size=target.shape[1:], mode=\"bilinear\", align_corners=False)\n",
    "                    .squeeze(dim=1)\n",
    "                    .cpu()\n",
    "                    .numpy(),\n",
    "                    target,\n",
    "                )\n",
    "            pbar.set_description(get_val(metrics)[1])\n",
    "    vals, _ = get_val(metrics)\n",
    "    print(\"----\" * 5)\n",
    "    return vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YtPmGx3j2VKk"
   },
   "source": [
    "## Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5FpRg81N205P"
   },
   "outputs": [],
   "source": [
    "from utils import MeanIoU, RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ozf0W-0I1yVc"
   },
   "outputs": [],
   "source": [
    "crop_size = 400\n",
    "batch_size = 4\n",
    "val_batch_size = 4\n",
    "val_every = 5\n",
    "loss_coeffs = (0.5, 0.5)\n",
    "\n",
    "#TODO: Define a Training Loop! (Good Luck!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1MCe8myT4wtk"
   },
   "source": [
    "# Inference Challenge\n",
    "\n",
    "Now that your model is trained and checkpoint saved, try and **load an image from the test dataset and run your model on it**. Print the FPS.\n",
    "<p>\n",
    "\n",
    "**MEGA POINTS** ‚Äî Load a video, and **implement a video pipeline** as we did on the previous workshop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7PR-brzyAzIH"
   },
   "outputs": [],
   "source": [
    "#Good Luck! If you have any good result, send it to jeremy@thinkautonomous.ai directly!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "MTL_Train_Home_S.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

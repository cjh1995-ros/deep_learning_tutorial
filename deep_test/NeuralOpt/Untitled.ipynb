{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c233b7e3-882f-415a-ba5f-464300283487",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33e020cb-a315-473b-8459-a0878676ab8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download training data from open datasets.\n",
    "path = \"/home/mint-lab/cjh_ws/ThinkAutonomous\"\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\".\",\n",
    "    train=True,\n",
    "    download=False,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "# Download test data from open datasets.\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\".\",\n",
    "    train=False,\n",
    "    download=False,\n",
    "    transform=ToTensor(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8abf3173-f793-43ad-b148-327c2647e055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "print(test_data[0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "968857b3-2b64-4945-960c-4a8234f24ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
      "Shape of y: torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "NUM_WORKERS = 1 \n",
    "TEST_BATCH_SIZE  = 64\n",
    "TRAIN_BATCH_SIZE = 16\n",
    "VALID_SIZE = 0.2\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=TRAIN_BATCH_SIZE)\n",
    "test_dataloader = DataLoader(test_data, batch_size=TEST_BATCH_SIZE)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "597c5e1f-8840-4132-9b92-359e77180682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "class LinearNeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "class ConvNueralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 28, kernel_size=3, stride=1)\n",
    "        self.conv2 = nn.Conv2d(28, 14, kernel_size=3, stride=1)\n",
    "        self.conv3 = nn.Conv2d(10, 10, kernel_size=3, stride=1)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear = nn.Linear(100, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = nn.ReLU(self.con1(x))\n",
    "        x = nn.ReLU(self.con2(x))\n",
    "        x = nn.ReLU(self.con3(x))\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "linear_model = LinearNeuralNetwork().to(device)\n",
    "conv_model = ConvNueralNetwork().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4fb19425-1328-4122-b2ac-b194365c1ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv2d = nn.Conv2d(1, 28, kernel_size=3, stride=1)\n",
    "input_image = torch.rand(1, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66c17375-3969-4695-aa4b-f656d0dc8b01",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected 4-dimensional input for 4-dimensional weight [28, 1, 3, 3], but got 3-dimensional input of size [1, 28, 28] instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_16026/336734511.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/dl_ws/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dl_ws/lib/python3.9/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dl_ws/lib/python3.9/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    440\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 442\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    443\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected 4-dimensional input for 4-dimensional weight [28, 1, 3, 3], but got 3-dimensional input of size [1, 28, 28] instead"
     ]
    }
   ],
   "source": [
    "print(conv2d(input_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5cc1091b-e33a-49fb-be34-8729599475ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "16edbcfc-41d8-4cd3-aa50-249f4f10673a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6797, 0.7743, 0.4189, 0.0660, 0.5070, 0.6278, 0.3472, 0.4449, 0.4058,\n",
       "        0.7808])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7a89e182-2186-46d2-896a-bf3001d6aece",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6797, 0.7743, 0.4189, 0.0660, 0.5070, 0.6278, 0.3472, 0.4449, 0.4058,\n",
       "         0.7808]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.view(-1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "01d1ce5c-45a0-40c4-9128-7b219a98d0ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3751, 0.9232, 0.9659, 0.4996, 0.3391, 0.7702, 0.0654, 0.0862, 0.7560,\n",
       "         0.7869]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "75a8c7d7-7490-4cce-8199-fd2b26072603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function view:\n",
      "\n",
      "view(...) method of torch.Tensor instance\n",
      "    view(*shape) -> Tensor\n",
      "    \n",
      "    Returns a new tensor with the same data as the :attr:`self` tensor but of a\n",
      "    different :attr:`shape`.\n",
      "    \n",
      "    The returned tensor shares the same data and must have the same number\n",
      "    of elements, but may have a different size. For a tensor to be viewed, the new\n",
      "    view size must be compatible with its original size and stride, i.e., each new\n",
      "    view dimension must either be a subspace of an original dimension, or only span\n",
      "    across original dimensions :math:`d, d+1, \\dots, d+k` that satisfy the following\n",
      "    contiguity-like condition that :math:`\\forall i = d, \\dots, d+k-1`,\n",
      "    \n",
      "    .. math::\n",
      "    \n",
      "      \\text{stride}[i] = \\text{stride}[i+1] \\times \\text{size}[i+1]\n",
      "    \n",
      "    Otherwise, it will not be possible to view :attr:`self` tensor as :attr:`shape`\n",
      "    without copying it (e.g., via :meth:`contiguous`). When it is unclear whether a\n",
      "    :meth:`view` can be performed, it is advisable to use :meth:`reshape`, which\n",
      "    returns a view if the shapes are compatible, and copies (equivalent to calling\n",
      "    :meth:`contiguous`) otherwise.\n",
      "    \n",
      "    Args:\n",
      "        shape (torch.Size or int...): the desired size\n",
      "    \n",
      "    Example::\n",
      "    \n",
      "        >>> x = torch.randn(4, 4)\n",
      "        >>> x.size()\n",
      "        torch.Size([4, 4])\n",
      "        >>> y = x.view(16)\n",
      "        >>> y.size()\n",
      "        torch.Size([16])\n",
      "        >>> z = x.view(-1, 8)  # the size -1 is inferred from other dimensions\n",
      "        >>> z.size()\n",
      "        torch.Size([2, 8])\n",
      "    \n",
      "        >>> a = torch.randn(1, 2, 3, 4)\n",
      "        >>> a.size()\n",
      "        torch.Size([1, 2, 3, 4])\n",
      "        >>> b = a.transpose(1, 2)  # Swaps 2nd and 3rd dimension\n",
      "        >>> b.size()\n",
      "        torch.Size([1, 3, 2, 4])\n",
      "        >>> c = a.view(1, 3, 2, 4)  # Does not change tensor layout in memory\n",
      "        >>> c.size()\n",
      "        torch.Size([1, 3, 2, 4])\n",
      "        >>> torch.equal(b, c)\n",
      "        False\n",
      "    \n",
      "    \n",
      "    .. method:: view(dtype) -> Tensor\n",
      "       :noindex:\n",
      "    \n",
      "    Returns a new tensor with the same data as the :attr:`self` tensor but of a\n",
      "    different :attr:`dtype`. :attr:`dtype` must have the same number of bytes per\n",
      "    element as :attr:`self`'s dtype.\n",
      "    \n",
      "    .. warning::\n",
      "    \n",
      "        This overload is not supported by TorchScript, and using it in a Torchscript\n",
      "        program will cause undefined behavior.\n",
      "    \n",
      "    \n",
      "    Args:\n",
      "        dtype (:class:`torch.dtype`): the desired dtype\n",
      "    \n",
      "    Example::\n",
      "    \n",
      "        >>> x = torch.randn(4, 4)\n",
      "        >>> x\n",
      "        tensor([[ 0.9482, -0.0310,  1.4999, -0.5316],\n",
      "                [-0.1520,  0.7472,  0.5617, -0.8649],\n",
      "                [-2.4724, -0.0334, -0.2976, -0.8499],\n",
      "                [-0.2109,  1.9913, -0.9607, -0.6123]])\n",
      "        >>> x.dtype\n",
      "        torch.float32\n",
      "    \n",
      "        >>> y = x.view(torch.int32)\n",
      "        >>> y\n",
      "        tensor([[ 1064483442, -1124191867,  1069546515, -1089989247],\n",
      "                [-1105482831,  1061112040,  1057999968, -1084397505],\n",
      "                [-1071760287, -1123489973, -1097310419, -1084649136],\n",
      "                [-1101533110,  1073668768, -1082790149, -1088634448]],\n",
      "            dtype=torch.int32)\n",
      "        >>> y[0, 0] = 1000000000\n",
      "        >>> x\n",
      "        tensor([[ 0.0047, -0.0310,  1.4999, -0.5316],\n",
      "                [-0.1520,  0.7472,  0.5617, -0.8649],\n",
      "                [-2.4724, -0.0334, -0.2976, -0.8499],\n",
      "                [-0.2109,  1.9913, -0.9607, -0.6123]])\n",
      "    \n",
      "        >>> x.view(torch.int16)\n",
      "        Traceback (most recent call last):\n",
      "          File \"<stdin>\", line 1, in <module>\n",
      "        RuntimeError: Viewing a tensor as a new dtype with a different number of bytes per element is not supported.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(torch.Tensor().view)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924fb0c6-69e0-450e-9e86-1ccc632d5689",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.Tensor().view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340fe83c-c625-4462-8b6c-189926e527db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-27 21:34:25.505960: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-27 21:34:25.511434: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-27 21:34:25.512048: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "\u001b[0m\u001b[38;5;8m[\u001b[0m2022-04-27T12:34:25Z \u001b[0m\u001b[1m\u001b[31mERROR\u001b[0m rustboard_core::disk_logdir\u001b[0m\u001b[38;5;8m]\u001b[0m While walking log directory: IO error for operation on runs: No such file or directory (os error 2)\n",
      "\n",
      "NOTE: Using experimental fast data loading logic. To disable, pass\n",
      "    \"--load_fast=false\" and report issues on GitHub. More details:\n",
      "    https://github.com/tensorflow/tensorboard/issues/4784\n",
      "\n",
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
      "TensorBoard 2.6.0 at http://localhost:6006/ (Press CTRL+C to quit)\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir=runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d93557-6080-451c-925f-049e7ca286d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c938e31",
   "metadata": {},
   "source": [
    "# Model Metrics Workshop\n",
    "어떻게 하면 우리의 모델에서 아래의 내용을 알아낼 수 있는지 살펴본다.\n",
    "\n",
    "1. The size of a model (in RAM)\n",
    "2. The size of a model (when stored as a weight file)\n",
    "3. The inference time\n",
    "4. The FLOPs, MACs, and number of parameters\n",
    "\n",
    "추후에 우리 모델이 제대로 최적화 되었는지 확인하기 위한 툴로 생각하면 좋겠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "965d8288",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch import nn\n",
    "import torch, os, time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2662ba0",
   "metadata": {},
   "source": [
    "## Get a Model\n",
    "가장 기본이 되는 모델을 불러온다. resnet18, 34, 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b290b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet18, resnet34, resnet50\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_list = [resnet18(pretrained=False).to(device), resnet34(pretrained=False).to(device), resnet50(pretrained=False).to(device)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b613f1",
   "metadata": {},
   "source": [
    "모델에 넣을 dummy input을 생성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3f937ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = torch.randn(2, 3, 224, 224).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3088dec7",
   "metadata": {},
   "source": [
    "# Get the Size of Model\n",
    "\n",
    "저장된 모델이 RAM에 load 되었을 때의 사이즈를 알아본다.\n",
    "\n",
    "> - Parameters: Weights & Biases\n",
    "> - Buffers: Additional tensors used to track information such as the mean and standard deviation of a batchnorm.\n",
    "\n",
    "Buffers는 batchnorm 과정을 통해 생기는 deviation이나 mean 같은 정보를 추적하는데 사용되는 tensor를 의미한다...? __저게 뭐지?__\n",
    "\n",
    "추가적인 내용 조사 드가자 --> 예를 들어서 일반 변수가 램에 올라갔을 때의 경우를 분석한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1015c7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_size(model):\n",
    "    size_of_parameters = sum([param.element_size() * param.nelement() for param in model.parameters()])\n",
    "    size_of_buffers = sum([buf.element_size() * buf.nelement() for buf in model.buffers()])\n",
    "    model_size = size_of_parameters + size_of_buffers #Bytes\n",
    "    KILOBYTE_TO_BYTE = 1024\n",
    "    MEGABYTE_TO_KILOBYTE = 1024\n",
    "    model_size = model_size / (KILOBYTE_TO_BYTE * MEGABYTE_TO_KILOBYTE) #MegaBytes\n",
    "    return model_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7d933b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the model = 44.629 Mb\n",
      "Size of the model = 83.217 Mb\n",
      "Size of the model = 97.695 Mb\n"
     ]
    }
   ],
   "source": [
    "for model in model_list:\n",
    "  model_size = get_model_size(model)\n",
    "  print(f\"Size of the model = {round(model_size,3)} Mb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e71980",
   "metadata": {},
   "source": [
    "# Get the inference time\n",
    "이제 추론 시간을 계산한다. 그냥 계산 전후 시간의 차이를 계산한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "301a50a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference time = 3.5304ms\n",
      "Inference time = 5.3024ms\n",
      "Inference time = 7.8869ms\n"
     ]
    }
   ],
   "source": [
    "for model in model_list: \n",
    "    model = model.to(\"cpu\")\n",
    "    dummy_input = dummy_input.to(\"cpu\")\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        start_time = time.time()\n",
    "        _ = model(dummy_input)\n",
    "        end_time = time.time()\n",
    "        t = (end_time - start_time)*100\n",
    "        print(f\"Inference time = {t:.4f}ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8c73d007",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inference_time(model : nn.Module, dummy_input : torch.Tensor, device:torch.device, n_iters = 10) -> float:\n",
    "    \"\"\"Function to calculate inference time of model on given input\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): input model\n",
    "        input (torch.Tensor): sample input of valid shape\n",
    "        device (torch.device): compute device as in CPU, GPU, TPU]. Defaults to 'cpu'.\n",
    "        nIters (int, optional): number of iterations over which to find avg inference time. Defaults to 10.\n",
    "\n",
    "    Returns:\n",
    "        avg_inference_time (float): Avg inference time for `input` over `nIters` for `model`\n",
    "    \"\"\"   \n",
    "    # initialize default value\n",
    "    avg_inference_time = np.inf # 무한대의 양수\n",
    "\n",
    "    # check for GPU availability and user option\n",
    "    checkForGPU = False\n",
    "    if torch.cuda.is_available() == True:\n",
    "        if 'cuda' in str(device):\n",
    "            checkForGPU = True\n",
    "\n",
    "    # check for zero input\n",
    "    if n_iters > 0:\n",
    "        # move to target device        \n",
    "        model = model.to(device)\n",
    "        dummy_input = dummy_input.to(device)\n",
    "\n",
    "        # change model to inference mode\n",
    "        model.eval()\n",
    "\n",
    "        # find the avg time take for forward pass\n",
    "        with torch.no_grad():\n",
    "            start_time = time.time()\n",
    "            for _ in range(n_iters):\n",
    "                _ = model(dummy_input)\n",
    "\n",
    "                # wait for cuda to finish (cuda is asynchronous!)\n",
    "                if checkForGPU == True:\n",
    "                    torch.cuda.synchronize()\n",
    "            endTime = time.time()\n",
    "        \n",
    "        elapsedTime = endTime - start_time\n",
    "        batch_size = dummy_input.size()[0]\n",
    "        avg_inference_time = elapsedTime / (batch_size *  n_iters)\n",
    "    return avg_inference_time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "09594ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single batch inference Time of model = 0.044 seconds\n",
      "Single batch inference Time of model = 0.002 seconds\n",
      "Single batch inference Time of model = 0.004 seconds\n"
     ]
    }
   ],
   "source": [
    "for model in model_list:\n",
    "    inference_time = get_inference_time(model, dummy_input, device) # in seconds\n",
    "    print(f\"Single batch inference Time of model = {round(inference_time,3)} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d2aadc",
   "metadata": {},
   "source": [
    "# Calculate the size of the model files\n",
    "그냥 모델의 사이즈 확인하는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f089e35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_file_size(model : nn.Module) -> float:\n",
    "    \"\"\"function returns size of model state dict in MB \n",
    "    Args:\n",
    "        model (nn.Module): input model\n",
    "    Returns:\n",
    "        modelFileSize (float): size of model state dict in MB\n",
    "    \"\"\"\n",
    "    torch.save(model.state_dict(), \"model.p\")\n",
    "    model_file_size = os.path.getsize(\"model.p\")/1e6\n",
    "    os.remove('model.p') # 잠깐 확인하는 용도\n",
    "    return model_file_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b3c02d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the model file = 46.836 Mb\n",
      "Size of the model file = 87.33 Mb\n",
      "Size of the model file = 102.545 Mb\n"
     ]
    }
   ],
   "source": [
    "for model in model_list:\n",
    "    model_file_size = get_model_file_size(model)\n",
    "    print(f\"Size of the model file = {round(model_file_size,3)} Mb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03dae457",
   "metadata": {},
   "source": [
    "# Calculate FLOPs, FLOPS and MACs\n",
    "\n",
    "- **FLOPs** means Floating Point Operations — it's the number of operations being run.\n",
    "- **FLOPS** means Floating Point Operations per Second — it's a hardware thing. The better your hardware, the more operations it can do.\n",
    "- **MACs** means Multiply-Accumulate Computations — it's a combination of an addition and a multiplication (Input*Weight + Bias is a good example of it).\n",
    "As a rule: 1 MAC = 2 FLOPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "76a0f05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('thop_library')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4c7733a1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'thop'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_74968/2588378489.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mthop\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprofile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'thop'"
     ]
    }
   ],
   "source": [
    "from thop import profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c2483d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mint-lab/cjh_ws/ThinkAutonomous/NeuralOpt\r\n"
     ]
    }
   ],
   "source": [
    "def get_metrics(model, input):\n",
    "    MACs, params = profile(model, inputs=(input,), verbose=False)\n",
    "    FLOPs = 2*MACs\n",
    "    return MACs* 1e-6, FLOPs*1e-6, params*1e-6\n",
    "\n",
    "for model in model_list:\n",
    "    input = input.to(device)\n",
    "    MMACs, MFLOPs, Mparams = get_metrics(model, input)\n",
    "    print(f\"{round(MMACs,3)} MMACs, {round(MFLOPs,3)} MFLOPs and {round(Mparams,3)} M parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8674b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

# Autogenerated by onnx-pytorch.

import glob
import os
import math

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision


class Model(nn.Module):
  def __init__(self):
    super(Model, self).__init__()
    self._vars = nn.ParameterDict()
    self._regularizer_params = []
    for b in glob.glob(
        os.path.join(os.path.dirname(__file__), "variables", "*.npy")):
      v = torch.from_numpy(np.load(b))
      requires_grad = v.dtype.is_floating_point or v.dtype.is_complex
      self._vars[os.path.basename(b)[:-4]] = nn.Parameter(v, requires_grad=requires_grad)
    self.n_Conv_0 = nn.Conv2d(**{'groups': 1, 'dilation': [1, 1], 'out_channels': 64, 'padding': [2, 2], 'kernel_size': (5, 5), 'stride': [1, 1], 'in_channels': 1, 'bias': True})
    self.n_Conv_0.weight.data = self._vars["conv1_weight"]
    self.n_Conv_0.bias.data = self._vars["conv1_bias"]
    self.n_Conv_2 = nn.Conv2d(**{'groups': 1, 'dilation': [1, 1], 'out_channels': 64, 'padding': [1, 1], 'kernel_size': (3, 3), 'stride': [1, 1], 'in_channels': 64, 'bias': True})
    self.n_Conv_2.weight.data = self._vars["conv2_weight"]
    self.n_Conv_2.bias.data = self._vars["conv2_bias"]
    self.n_Conv_4 = nn.Conv2d(**{'groups': 1, 'dilation': [1, 1], 'out_channels': 32, 'padding': [1, 1], 'kernel_size': (3, 3), 'stride': [1, 1], 'in_channels': 64, 'bias': True})
    self.n_Conv_4.weight.data = self._vars["conv3_weight"]
    self.n_Conv_4.bias.data = self._vars["conv3_bias"]
    self.n_Conv_6 = nn.Conv2d(**{'groups': 1, 'dilation': [1, 1], 'out_channels': 9, 'padding': [1, 1], 'kernel_size': (3, 3), 'stride': [1, 1], 'in_channels': 32, 'bias': True})
    self.n_Conv_6.weight.data = self._vars["conv4_weight"]
    self.n_Conv_6.bias.data = self._vars["conv4_bias"]

  def forward(self, *inputs):
    input, = inputs
    t_9 = self.n_Conv_0(input)
    t_10 = F.relu(t_9)
    t_11 = self.n_Conv_2(t_10)
    t_12 = F.relu(t_11)
    t_13 = self.n_Conv_4(t_12)
    t_14 = F.relu(t_13)
    t_15 = self.n_Conv_6(t_14)
    t_16 = self._vars['t_16']
    t_17 = torch.reshape(t_15, [s if s != 0 else t_15.shape[i] for i, s in enumerate(self._vars["t_16"])])
    t_18 = t_17.permute(*[0, 1, 4, 2, 5, 3])
    t_19 = self._vars['t_19']
    output = torch.reshape(t_18, [s if s != 0 else t_18.shape[i] for i, s in enumerate(self._vars["t_19"])])
    return output

  def compatible_auto_pad(self, input, kernel_spatial_shape, nn_mod, auto_pad=None, **kwargs):
    input_spatial_shape = input.shape[2:]
    d = len(input_spatial_shape)
    strides = nn_mod.stride
    dilations = nn_mod.dilation
    output_spatial_shape = [math.ceil(float(l) / float(r)) for l, r in zip(input.shape[2:], strides)]
    pt_padding = [0] * 2 * d
    pad_shape = [0] * d
    for i in range(d):
      pad_shape[i] = (output_spatial_shape[i] - 1) * strides[i] + ((kernel_spatial_shape[i] - 1) * dilations[i] + 1) - input_spatial_shape[i]
      mean = pad_shape[i] // 2
      if auto_pad == b"SAME_UPPER":
        l, r = pad_shape[i] - mean, mean
      else:
        l, r = mean, pad_shape[i] - mean
      pt_padding.insert(0, r)
      pt_padding.insert(0, l)
    return F.pad(input, pt_padding)

@torch.no_grad()
def test_run_model(inputs=[torch.from_numpy(np.random.randn(*[1, 1, 224, 224]).astype(np.float32))]):
  model = Model()
  model.eval()
  rs = model(*inputs)
  print(rs)
  return rs

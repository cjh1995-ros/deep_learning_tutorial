{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89886515-726b-4c7f-9136-64a6669deb8d",
   "metadata": {},
   "source": [
    "# Pytorch Basic Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c863023-0dfc-4938-acab-a5d64495d306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "277829a2eb3b4b1692eacff77eaeaf8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26421880 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6468eb6cb0a44f00beac381212de5d3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29515 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5be146b99da34506b9a549c01fcf8a82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4422102 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8d4e92671c04b33ba625fbb5422c184",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5148 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7eeade-baf3-4350-a1ae-4f1596b903d1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Dataset to Dataloader\n",
    "자동화된 batch, sampling, shuffle 및 multiprocess data loading 가능."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ad114a9-c5ae-4506-a7f8-d360e3930a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
      "Shape of y torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# 데이터로더(dataloader) 객체의 각 요소는 64개의 특징(feature)과 정답(label)을 묶음(batch)으로 반환\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d608db-1107-4c5b-bb9c-78ae625256e7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0924973b-82c2-4415-9756-8a398901b8e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "MyNN(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "class MyNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "my_model = MyNN().to(device)\n",
    "print(my_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff615cb9-8170-400a-a163-758c7dfeca04",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model Optimizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "854a357c-3004-4a20-b330-46c12b032175",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(my_model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ff0851a-ad1e-4d21-b296-745974324567",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        \n",
    "        # 예측 오류 계산\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        \n",
    "        # 역전파\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "006c704d-822e-4839-8653-9b0da247de42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9db943d-6043-4985-bb50-5b27c80c94ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.294224  [    0/60000]\n",
      "loss: 2.263263  [ 6400/60000]\n",
      "loss: 2.218781  [12800/60000]\n",
      "loss: 2.220002  [19200/60000]\n",
      "loss: 2.166966  [25600/60000]\n",
      "loss: 2.103980  [32000/60000]\n",
      "loss: 2.125259  [38400/60000]\n",
      "loss: 2.038337  [44800/60000]\n",
      "loss: 2.048050  [51200/60000]\n",
      "loss: 1.971260  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 52.6%, Avg loss: 1.960892 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 1.987887  [    0/60000]\n",
      "loss: 1.942183  [ 6400/60000]\n",
      "loss: 1.827266  [12800/60000]\n",
      "loss: 1.863031  [19200/60000]\n",
      "loss: 1.744087  [25600/60000]\n",
      "loss: 1.684538  [32000/60000]\n",
      "loss: 1.703871  [38400/60000]\n",
      "loss: 1.589594  [44800/60000]\n",
      "loss: 1.617865  [51200/60000]\n",
      "loss: 1.499824  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.8%, Avg loss: 1.517931 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.587660  [    0/60000]\n",
      "loss: 1.539493  [ 6400/60000]\n",
      "loss: 1.386596  [12800/60000]\n",
      "loss: 1.454606  [19200/60000]\n",
      "loss: 1.330862  [25600/60000]\n",
      "loss: 1.325501  [32000/60000]\n",
      "loss: 1.336004  [38400/60000]\n",
      "loss: 1.256888  [44800/60000]\n",
      "loss: 1.295185  [51200/60000]\n",
      "loss: 1.186945  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 64.0%, Avg loss: 1.217918 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.299293  [    0/60000]\n",
      "loss: 1.276390  [ 6400/60000]\n",
      "loss: 1.105461  [12800/60000]\n",
      "loss: 1.212924  [19200/60000]\n",
      "loss: 1.084429  [25600/60000]\n",
      "loss: 1.107227  [32000/60000]\n",
      "loss: 1.134285  [38400/60000]\n",
      "loss: 1.068493  [44800/60000]\n",
      "loss: 1.109045  [51200/60000]\n",
      "loss: 1.023063  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 65.2%, Avg loss: 1.047858 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.121049  [    0/60000]\n",
      "loss: 1.122985  [ 6400/60000]\n",
      "loss: 0.931574  [12800/60000]\n",
      "loss: 1.074432  [19200/60000]\n",
      "loss: 0.944802  [25600/60000]\n",
      "loss: 0.971086  [32000/60000]\n",
      "loss: 1.020127  [38400/60000]\n",
      "loss: 0.957104  [44800/60000]\n",
      "loss: 0.994935  [51200/60000]\n",
      "loss: 0.927206  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 66.6%, Avg loss: 0.944697 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, my_model, loss_fn, optimizer)\n",
    "    test(test_dataloader, my_model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8d7d5b-9974-428f-b7dd-ad46056ee92c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Save/Load Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "26c80787-7acb-4960-a2b6-5285bae1f7ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Pytorch Model State to model.pth\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "print(\"Saved Pytorch Model State to model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73d78e72-d0fd-4fcd-b0ee-00a2044f2713",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model = MyNN()\n",
    "test_model.load_state_dict(torch.load(\"model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6441c77f-12b9-4109-a17e-e38bc9046164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Trouser, Actual Ankle boot\n"
     ]
    }
   ],
   "source": [
    "classes = [\n",
    "    \"T-shirt/top\",\n",
    "    \"Trouser\",\n",
    "    \"Pullover\",\n",
    "    \"Dress\",\n",
    "    \"Coat\",\n",
    "    \"Sandal\",\n",
    "    \"Shirt\",\n",
    "    \"Sneaker\",\n",
    "    \"Bag\",\n",
    "    \"Ankle boot\",\n",
    "]\n",
    "\n",
    "model.eval()\n",
    "x, y = test_data[0][0], test_data[0][1]\n",
    "with torch.no_grad():\n",
    "    pred = model(x)\n",
    "    predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
    "    print(f\"Predicted {predicted}, Actual {actual}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2410e7de-37d2-4719-a1dc-d7067d3bf6f5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Exporting a Model from PyTorch to ONNX and Running it using ONNX Runtime\n",
    "\n",
    "* torch.onnx.export() 함수를 통해 onnx 파일을 만든다.\n",
    "* onnx 라이브러리를 통해 model을 load 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d6ea408e-6829-49ae-ad33-1450a7871d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.onnx as onnx\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "import io\n",
    "import numpy as np\n",
    "import torch.nn.init as init\n",
    "\n",
    "class SuperResolutionNet(nn.Module):\n",
    "    def __init__(self, upscale_factor, inplace=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=inplace)\n",
    "        self.conv1 = nn.Conv2d(1, 64, (5, 5), (1, 1), (2, 2))\n",
    "        self.conv2 = nn.Conv2d(64, 64, (3, 3), (1, 1), (1, 1))\n",
    "        self.conv3 = nn.Conv2d(64, 32, (3, 3), (1, 1), (1, 1))\n",
    "        self.conv4 = nn.Conv2d(32, upscale_factor ** 2, (3, 3), (1, 1), (1, 1))\n",
    "        self.pixel_shuffle = nn.PixelShuffle(upscale_factor)\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.relu(self.conv3(x))\n",
    "        x = self.pixel_shuffle(self.conv4(x))\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        init.orthogonal_(self.conv1.weight, init.calculate_gain('relu'))\n",
    "        init.orthogonal_(self.conv2.weight, init.calculate_gain('relu'))\n",
    "        init.orthogonal_(self.conv3.weight, init.calculate_gain('relu'))\n",
    "        init.orthogonal_(self.conv4.weight)\n",
    "\n",
    "# Create the super-resolution model by using the above model definition.\n",
    "torch_model = SuperResolutionNet(upscale_factor=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "59579d43-f99c-4878-b061-09b3d3ffc40d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://s3.amazonaws.com/pytorch/test_data/export/superres_epoch100-44c6958e.pth\" to /home/mint-lab/.cache/torch/hub/checkpoints/superres_epoch100-44c6958e.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cd6bf148d90459baf4e88b2a259ebfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/234k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "SuperResolutionNet(\n",
       "  (relu): ReLU()\n",
       "  (conv1): Conv2d(1, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv3): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv4): Conv2d(32, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pixel_shuffle): PixelShuffle(upscale_factor=3)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pretrained model weights\n",
    "model_url = 'https://s3.amazonaws.com/pytorch/test_data/export/superres_epoch100-44c6958e.pth'\n",
    "batch_size = 1    # just a random number\n",
    "\n",
    "# Initialize model with the pretrained weights\n",
    "map_location = lambda storage, loc: storage\n",
    "if torch.cuda.is_available():\n",
    "    map_location = None\n",
    "torch_model.load_state_dict(model_zoo.load_url(model_url, map_location=map_location))\n",
    "\n",
    "# set the model to inference mode\n",
    "torch_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "55a66551-18a0-4c93-95f4-1e966fdcf0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input to the model\n",
    "x = torch.randn(batch_size, 1, 224, 224, requires_grad=True)\n",
    "torch_out = torch_model(x)\n",
    "\n",
    "# Export the model\n",
    "torch.onnx.export(torch_model,               # model being run\n",
    "                  x,                         # model input (or a tuple for multiple inputs)\n",
    "                  \"super_resolution.onnx\",   # where to save the model (can be a file or file-like object)\n",
    "                  export_params=True,        # store the trained parameter weights inside the model file\n",
    "                  opset_version=10,          # the ONNX version to export the model to\n",
    "                  do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "                  input_names = ['input'],   # the model's input names\n",
    "                  output_names = ['output'], # the model's output names\n",
    "                  dynamic_axes={'input' : {0 : 'batch_size'},    # variable length axes\n",
    "                                'output' : {0 : 'batch_size'}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "884e1fe3-e0df-4677-a4b5-11a57983a1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "\n",
    "onnx_model = onnx.load(\"super_resolution.onnx\")\n",
    "onnx.checker.check_model(onnx_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6ac35584-3b45-49d9-becb-d137e05d51de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported model has been tested with ONNXRuntime, and the result looks good!\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime\n",
    "\n",
    "ort_session = onnxruntime.InferenceSession(\"super_resolution.onnx\")\n",
    "\n",
    "def to_numpy(tensor):\n",
    "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
    "\n",
    "# compute ONNX Runtime output prediction\n",
    "ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(x)}\n",
    "ort_outs = ort_session.run(None, ort_inputs)\n",
    "\n",
    "# compare ONNX Runtime and PyTorch results\n",
    "np.testing.assert_allclose(to_numpy(torch_out), ort_outs[0], rtol=1e-03, atol=1e-05)\n",
    "\n",
    "print(\"Exported model has been tested with ONNXRuntime, and the result looks good!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d06def96-080f-4111-bd84-865d309cac1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.2157, 0.1961, 0.1922,  ..., 0.5294, 0.5569, 0.5725],\n",
       "          [0.2039, 0.1922, 0.1922,  ..., 0.5333, 0.5529, 0.5686],\n",
       "          [0.2000, 0.1843, 0.1843,  ..., 0.5216, 0.5373, 0.5490],\n",
       "          ...,\n",
       "          [0.6667, 0.6745, 0.6392,  ..., 0.6902, 0.6667, 0.6078],\n",
       "          [0.6392, 0.6431, 0.6235,  ..., 0.8000, 0.7608, 0.6745],\n",
       "          [0.6392, 0.6353, 0.6510,  ..., 0.8118, 0.7686, 0.6667]]]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "img = Image.open(\"./_static/img/cat.jpg\")\n",
    "\n",
    "resize = transforms.Resize([224, 224])\n",
    "img = resize(img)\n",
    "\n",
    "img_ycbcr = img.convert('YCbCr')\n",
    "img_y, img_cb, img_cr = img_ycbcr.split()\n",
    "\n",
    "to_tensor = transforms.ToTensor()\n",
    "img_y = to_tensor(img_y)\n",
    "img_y.unsqueeze_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9c42db33-b8f4-46e9-8eeb-cd633183449a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(img_y)}\n",
    "ort_outs = ort_session.run(None, ort_inputs)\n",
    "img_out_y = ort_outs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "601ed4f2-7185-45cb-a6c7-8f77b2581173",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_out_y = Image.fromarray(np.uint8((img_out_y[0] * 255.0).clip(0, 255)[0]), mode='L')\n",
    "\n",
    "# get the output image follow post-processing step from PyTorch implementation\n",
    "final_img = Image.merge(\n",
    "    \"YCbCr\", [\n",
    "        img_out_y,\n",
    "        img_cb.resize(img_out_y.size, Image.BICUBIC),\n",
    "        img_cr.resize(img_out_y.size, Image.BICUBIC),\n",
    "    ]).convert(\"RGB\")\n",
    "\n",
    "# Save the image, we will compare this with the output image from mobile device\n",
    "final_img.save(\"./_static/img/cat_superres_with_ort.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ed0c92a2-590d-4235-a3c8-b82259539cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is valid!\n"
     ]
    }
   ],
   "source": [
    "# print('The model is:\\n{}'.format(onnx_model))\n",
    "\n",
    "# Check the model\n",
    "try:\n",
    "    onnx.checker.check_model(onnx_model)\n",
    "except onnx.checker.ValidationError as e:\n",
    "    print('The model is invalid: %s' % e)\n",
    "else:\n",
    "    print('The model is valid!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb54123d-fa0a-479e-81ee-0e944eb832b6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## NETRON 을 통한 ONNX 시각화\n",
    "\n",
    "```shell\n",
    "sudo apt update\n",
    "sudo apt install snapd\n",
    "```\n",
    "\n",
    "아쉽게도 **ONNX to Torch** 관련 내용은 찾을 수 없었음. 하지만 [git link](https://github.com/fumihwh/onnx-pytorch) 여기서 뭔가 비슷한 결과를 보여주긴 하는데, 살짝 아쉬운 느낌이 든다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3e5bc8a8-3d5b-4525-995e-1b36a4e1207c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Cannot get default value for dilations of Conv.\n",
      "WARNING:root:Cannot get default value for kernel_shape of Conv.\n",
      "WARNING:root:Cannot get default value for pads of Conv.\n",
      "WARNING:root:Cannot get default value for strides of Conv.\n",
      "WARNING:root:Cannot get default value for sparse_value of Constant.\n",
      "WARNING:root:Cannot get default value for value of Constant.\n",
      "WARNING:root:Cannot get default value for value_float of Constant.\n",
      "WARNING:root:Cannot get default value for value_floats of Constant.\n",
      "WARNING:root:Cannot get default value for value_int of Constant.\n",
      "WARNING:root:Cannot get default value for value_ints of Constant.\n",
      "WARNING:root:Cannot get default value for value_string of Constant.\n",
      "WARNING:root:Cannot get default value for value_strings of Constant.\n",
      "WARNING:root:Cannot get default value for perm of Transpose.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Autogenerated by onnx-pytorch.\n",
      "\n",
      "import glob\n",
      "import os\n",
      "import math\n",
      "\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "import torchvision\n",
      "\n",
      "\n",
      "class Model(nn.Module):\n",
      "  def __init__(self):\n",
      "    super(Model, self).__init__()\n",
      "    self._vars = nn.ParameterDict()\n",
      "    self._regularizer_params = []\n",
      "    for b in glob.glob(\n",
      "        os.path.join(os.path.dirname(__file__), \"variables\", \"*.npy\")):\n",
      "      v = torch.from_numpy(np.load(b))\n",
      "      requires_grad = v.dtype.is_floating_point or v.dtype.is_complex\n",
      "      self._vars[os.path.basename(b)[:-4]] = nn.Parameter(v, requires_grad=requires_grad)\n",
      "    self.n_Conv_0 = nn.Conv2d(**{'groups': 1, 'dilation': [1, 1], 'out_channels': 64, 'padding': [2, 2], 'kernel_size': (5, 5), 'stride': [1, 1], 'in_channels': 1, 'bias': True})\n",
      "    self.n_Conv_0.weight.data = self._vars[\"conv1_weight\"]\n",
      "    self.n_Conv_0.bias.data = self._vars[\"conv1_bias\"]\n",
      "    self.n_Conv_2 = nn.Conv2d(**{'groups': 1, 'dilation': [1, 1], 'out_channels': 64, 'padding': [1, 1], 'kernel_size': (3, 3), 'stride': [1, 1], 'in_channels': 64, 'bias': True})\n",
      "    self.n_Conv_2.weight.data = self._vars[\"conv2_weight\"]\n",
      "    self.n_Conv_2.bias.data = self._vars[\"conv2_bias\"]\n",
      "    self.n_Conv_4 = nn.Conv2d(**{'groups': 1, 'dilation': [1, 1], 'out_channels': 32, 'padding': [1, 1], 'kernel_size': (3, 3), 'stride': [1, 1], 'in_channels': 64, 'bias': True})\n",
      "    self.n_Conv_4.weight.data = self._vars[\"conv3_weight\"]\n",
      "    self.n_Conv_4.bias.data = self._vars[\"conv3_bias\"]\n",
      "    self.n_Conv_6 = nn.Conv2d(**{'groups': 1, 'dilation': [1, 1], 'out_channels': 9, 'padding': [1, 1], 'kernel_size': (3, 3), 'stride': [1, 1], 'in_channels': 32, 'bias': True})\n",
      "    self.n_Conv_6.weight.data = self._vars[\"conv4_weight\"]\n",
      "    self.n_Conv_6.bias.data = self._vars[\"conv4_bias\"]\n",
      "\n",
      "  def forward(self, *inputs):\n",
      "    input, = inputs\n",
      "    t_9 = self.n_Conv_0(input)\n",
      "    t_10 = F.relu(t_9)\n",
      "    t_11 = self.n_Conv_2(t_10)\n",
      "    t_12 = F.relu(t_11)\n",
      "    t_13 = self.n_Conv_4(t_12)\n",
      "    t_14 = F.relu(t_13)\n",
      "    t_15 = self.n_Conv_6(t_14)\n",
      "    t_16 = self._vars['t_16']\n",
      "    t_17 = torch.reshape(t_15, [s if s != 0 else t_15.shape[i] for i, s in enumerate(self._vars[\"t_16\"])])\n",
      "    t_18 = t_17.permute(*[0, 1, 4, 2, 5, 3])\n",
      "    t_19 = self._vars['t_19']\n",
      "    output = torch.reshape(t_18, [s if s != 0 else t_18.shape[i] for i, s in enumerate(self._vars[\"t_19\"])])\n",
      "    return output\n",
      "\n",
      "  def compatible_auto_pad(self, input, kernel_spatial_shape, nn_mod, auto_pad=None, **kwargs):\n",
      "    input_spatial_shape = input.shape[2:]\n",
      "    d = len(input_spatial_shape)\n",
      "    strides = nn_mod.stride\n",
      "    dilations = nn_mod.dilation\n",
      "    output_spatial_shape = [math.ceil(float(l) / float(r)) for l, r in zip(input.shape[2:], strides)]\n",
      "    pt_padding = [0] * 2 * d\n",
      "    pad_shape = [0] * d\n",
      "    for i in range(d):\n",
      "      pad_shape[i] = (output_spatial_shape[i] - 1) * strides[i] + ((kernel_spatial_shape[i] - 1) * dilations[i] + 1) - input_spatial_shape[i]\n",
      "      mean = pad_shape[i] // 2\n",
      "      if auto_pad == b\"SAME_UPPER\":\n",
      "        l, r = pad_shape[i] - mean, mean\n",
      "      else:\n",
      "        l, r = mean, pad_shape[i] - mean\n",
      "      pt_padding.insert(0, r)\n",
      "      pt_padding.insert(0, l)\n",
      "    return F.pad(input, pt_padding)\n",
      "\n",
      "@torch.no_grad()\n",
      "def test_run_model(inputs=[torch.from_numpy(np.random.randn(*[1, 1, 224, 224]).astype(np.float32))]):\n",
      "  model = Model()\n",
      "  model.eval()\n",
      "  rs = model(*inputs)\n",
      "  print(rs)\n",
      "  return rs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from onnx_pytorch import code_gen\n",
    "path = \"/home/mint-lab/cjh_ws/ThinkAutonomous\"\n",
    "code_gen.gen(path + \"/MyTutorial/super_resolution.onnx\", path + \"/MyTutorial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86152bbf-78d1-464e-b089-cf5a9a3d0bbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
